{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNif1Yfb4YkX9wNsW+pnxJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aneelmanan/Mycodes/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install river"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbxJDtTaGXE3",
        "outputId": "51ae62f2-9060-45e4-9ff9-16b3c8bc5f30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: river in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from river) (2.2.3)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from river) (1.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.3->river) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install creme"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RnKh8CMHD5g",
        "outputId": "f4b6bc49-77f7-4ad1-d8fa-9824d44a6306"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting creme\n",
            "  Downloading creme-0.6.1.tar.gz (524 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/524.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/524.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.7/524.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mmh3==2.5.1 (from creme)\n",
            "  Downloading mmh3-2.5.1.tar.gz (9.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.11/dist-packages (from creme) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from creme) (1.15.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from creme) (2.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->creme) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->creme) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->creme) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->creme) (1.17.0)\n",
            "Building wheels for collected packages: creme, mmh3\n",
            "  Building wheel for creme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for creme: filename=creme-0.6.1-cp311-cp311-linux_x86_64.whl size=1274652 sha256=c25434bf560437ac61b2d61c64e68b77f9f075c63c93e56d4bac8c6d5d7ffac4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/8d/e2/c284468e530880caaf1b1896798299b066889164f8ff4fcb90\n",
            "  Building wheel for mmh3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmh3: filename=mmh3-2.5.1-cp311-cp311-linux_x86_64.whl size=32588 sha256=4c9b6070c2852287a9271cb6e41a8f930fc818ad194fcb3b10f819cab5f888ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/28/56/6dbfc941755fd6ac305d432ed1241258c5b8039469ef2c62b4\n",
            "Successfully built creme mmh3\n",
            "Installing collected packages: mmh3, creme\n",
            "Successfully installed creme-0.6.1 mmh3-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import river\n",
        "print(dir(river))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F14FXfK_Hg3g",
        "outputId": "4c3daf42-a925-46fe-da48-e0c8d2eeadf8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'annotations', 'base', 'compose', 'covariance', 'datasets', 'drift', 'ensemble', 'linear_model', 'metrics', 'naive_bayes', 'optim', 'preprocessing', 'proba', 'sketch', 'stats', 'stream', 'tree', 'utils']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from river.linear_model import LogisticRegression\n",
        "from river.naive_bayes import MultinomialNB\n",
        "from river.feature_extraction import BagOfWords, TFIDF"
      ],
      "metadata": {
        "id": "B82AqJqWLV1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import importlib\n",
        "\n",
        "def get_all_attributes(package):\n",
        "    subpackages = []\n",
        "    submodules = []\n",
        "\n",
        "    for i in dir(package):\n",
        "        # Skip internal and unwanted attributes\n",
        "        if str(i) not in ['_all_', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'annotations', 'base', 'compose', 'covariance', 'datasets', 'drift', 'ensemble', 'linear_model', 'metrics', 'naive_bayes', 'optim', 'preprocessing', 'proba', 'sketch', 'stats', 'stream', 'tree', 'utils']:\n",
        "            subpackages.append(i)\n",
        "            try:\n",
        "                # Dynamically import the submodule using importlib\n",
        "                submodule = importlib.import_module(f\"{package.__name__}.{i}\")\n",
        "                res = [j for j in dir(submodule)]\n",
        "                submodules.append(res)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to import submodule {i}: {e}\")\n",
        "                submodules.append([])  # Append empty list if import fails\n",
        "\n",
        "    # Create a DataFrame and transpose it\n",
        "    df = pd.DataFrame(submodules)\n",
        "    df = df.T  # Transpose\n",
        "    df.columns = subpackages\n",
        "\n",
        "    # Drop NaN values if any\n",
        "    res_df = df.dropna()\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HmyGRG1IL8O3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import river  # Ensure you have the 'river' package installed\n",
        "\n",
        "df = get_all_attributes(river)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V5XAzeqOF3V",
        "outputId": "1e1940b1-7848-425b-9708-478e47048aaf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    feature_extraction\n",
            "0                  Agg\n",
            "1           BagOfWords\n",
            "2   PolynomialExtender\n",
            "3           RBFSampler\n",
            "4                TFIDF\n",
            "5            TargetAgg\n",
            "6              __all__\n",
            "7         __builtins__\n",
            "8           __cached__\n",
            "9              __doc__\n",
            "10            __file__\n",
            "11          __loader__\n",
            "12            __name__\n",
            "13         __package__\n",
            "14            __path__\n",
            "15            __spec__\n",
            "16                 agg\n",
            "17         annotations\n",
            "18       kernel_approx\n",
            "19                poly\n",
            "20           vectorize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "\n",
        "# Load your dataset (example data from your input)\n",
        "data = {\n",
        "    \"OPC\": [1, 1, 1, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95],\n",
        "    \"RCP\": [0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
        "    \"FA\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    \"LP\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"GGBFS\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"Temp\": [0, 0, 0, 100, 100, 100, 150, 150, 150, 200, 200],\n",
        "    \"SF\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"Sand\": [0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73],\n",
        "    \"Water\": [0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66],\n",
        "    \"F-AR\": [0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52],\n",
        "    \"Stress\": [37.5, 38.04, 37.8, 36.67, 36.2, 35.9, 31.96, 32.1, 31.8, 38.1, 37.9]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Create the model pipeline (features scaling + regression)\n",
        "model = preprocessing.StandardScaler() | linear_model.SGDRegressor()\n",
        "\n",
        "# Create a stream from the dataset\n",
        "stream_data = stream.iter_csv(df.to_csv(index=False), target=\"Stress\")\n",
        "\n",
        "# Metric to track performance\n",
        "metric = metrics.R2()\n",
        "\n",
        "# Train the model incrementally\n",
        "for x, y in stream_data:\n",
        "    model.learn_one(x, y)\n",
        "    metric.update(y, model.predict_one(x))\n",
        "\n",
        "# Print the R2 score (performance)\n",
        "print(f\"Model R^2 score: {metric.get()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "8gHrDD-RO-R1",
        "outputId": "8fea4a62-4e4a-432f-fa34-286915b4d2cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'river.linear_model' has no attribute 'SGDRegressor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-eee27f70a921>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Create the model pipeline (features scaling + regression)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGDRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Create a stream from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'river.linear_model' has no attribute 'SGDRegressor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "import tempfile\n",
        "\n",
        "# Load your dataset (example data from your input)\n",
        "data = {\n",
        "   data = {\n",
        "    \"OPC\": [1, 1, 1, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.45, 0.56, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.7, 0.7, 0.7, 0.7],\n",
        "    \"RCP\": [0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"FA\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.55, 0.34, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.3, 0.3, 0.3],\n",
        "    \"LP\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"GGBFS\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"Temp\": [0, 0, 0, 100, 100, 100, 150, 150, 150, 200, 200, 200, 250, 250, 250, 100, 100, 100, 150, 150, 150, 200, 200, 200, 250, 250, 250, 100, 100, 100, 150, 150, 150, 200, 200, 200, 250, 250, 250, 100, 100, 100],\n",
        "    \"SF\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "    \"Sand\": [0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.73, 0.36, 0.36, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.3, 0.3, 0.3],\n",
        "    \"Water\": [0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.66, 0.28, 0.31, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27, 0.27],\n",
        "    \"F-AR\": [0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.52, 0.73, 0.73, 0.67, 0.67, 0.67, 0.67, 0.67, 0.67, 0.52, 0.52, 0.52],\n",
        "    \"Stress\": [37.5, 38.04, 37.8, 36.67, 36.2, 35.9, 31.96, 32.1, 31.8, 38.1, 37.9, 38.2, 35.92, 36, 35.8, 36.07, 36.3, 36, 35.54, 35.8, 35.6, 39.87, 40, 39.7, 39.19, 39.5, 39.3],\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Ensure that the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Save the DataFrame to a temporary CSV file\n",
        "with tempfile.NamedTemporaryFile(delete=False, mode='w', newline='') as tmpfile:\n",
        "    df.to_csv(tmpfile.name, index=False)\n",
        "    tmpfile.close()  # We close to ensure the file is saved\n",
        "\n",
        "# Create the model pipeline (features scaling + regression)\n",
        "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
        "\n",
        "# Create a stream from the temporary CSV file\n",
        "stream_data = stream.iter_csv(tmpfile.name, target=\"Stress\")\n",
        "\n",
        "# Metric to track performance\n",
        "metric = metrics.R2()\n",
        "\n",
        "# Train the model incrementally\n",
        "for x, y in stream_data:\n",
        "    # Ensure that the data passed to `learn_one` is numeric\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    model.learn_one(x, y)\n",
        "    metric.update(y, model.predict_one(x))\n",
        "\n",
        "# Print the R2 score (performance)\n",
        "print(f\"Model R^2 score: {metric.get()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "XKa8LDakQF2d",
        "outputId": "ee0f255e-4ffb-4453-d670-08e54a766ef7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "'{' was never closed (<ipython-input-35-0028bbc8b9fc>, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-0028bbc8b9fc>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    data = {\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '{' was never closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "\n",
        "# File ID from your Google Drive link\n",
        "file_id = '1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "\n",
        "# Construct the file URL\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Define the local path where the file will be downloaded\n",
        "output_path = '/content/your_file.csv'  # Adjust the name as needed\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv(output_path)\n",
        "\n",
        "# Check the first few rows of the DataFrame\n",
        "df.head()\n",
        "\n",
        "# Create the model pipeline (features scaling + SGDRegressor)\n",
        "model = preprocessing.StandardScaler() | linear_model.SGDRegressor()\n",
        "\n",
        "# Create a stream from the CSV file for incremental learning\n",
        "stream_data = stream.iter_csv(output_path, target=\"Stress\")\n",
        "\n",
        "# Metric to track performance\n",
        "metric = metrics.R2()\n",
        "\n",
        "# Train the model incrementally\n",
        "for x, y in stream_data:\n",
        "    # Ensure that the data passed to `learn_one` is numeric\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    model.learn_one(x, y)  # Incremental learning on one instance\n",
        "    metric.update(y, model.predict_one(x))  # Update the metric after each prediction\n",
        "\n",
        "# Print the R^2 score (performance)\n",
        "print(f\"Model R^2 score: {metric.get()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "HyHfJ33SWHot",
        "outputId": "2b4b5bea-83c0-47dc-b54a-cc07f3942142"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n\nbut Gdown can't. Please check connections and permissions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_url_from_gdrive_confirmation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         raise FileURLRetrievalError(\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;34m\"Cannot retrieve the public link of the file. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d1cbb68b16ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Download the file from Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the CSV file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0murl_origin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             )\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mfilename_from_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "#!pip install gdown river pandas\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = '1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "\n",
        "# Construct the download URL using the file ID\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "output = 'data.csv'  # Save file as 'data.csv'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load the downloaded CSV into a DataFrame\n",
        "df = pd.read_csv(output)\n",
        "\n",
        "# Ensure that the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Save the DataFrame to a temporary CSV file (this step is unnecessary if you directly load it)\n",
        "tmpfile = '/tmp/data.csv'\n",
        "df.to_csv(tmpfile, index=False)\n",
        "\n",
        "# Create the model pipeline (features scaling + LinearRegression)\n",
        "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
        "\n",
        "# Create a stream from the CSV file\n",
        "stream_data = stream.iter_csv(tmpfile, target=\"Stress\")\n",
        "\n",
        "# Metric to track performance\n",
        "metric = metrics.R2()\n",
        "\n",
        "# Train the model incrementally\n",
        "for x, y in stream_data:\n",
        "    # Ensure that the data passed to `learn_one` is numeric\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    model.learn_one(x, y)\n",
        "    metric.update(y, model.predict_one(x))\n",
        "\n",
        "# Print the R2 score (performance)\n",
        "print(f\"Model R^2 score: {metric.get()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8kOEJ0NXaPs",
        "outputId": "3793d8af-8289-401c-bbd3-6b640f0cf9fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n",
            "To: /content/data.csv\n",
            "100%|██████████| 7.07k/7.07k [00:00<00:00, 12.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model R^2 score: 0.3583192628676791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-40-b4e68d8152e0>:25: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df = df.apply(pd.to_numeric, errors='ignore')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Step 1: Download dataset from Google Drive (using file ID)\n",
        "url = 'https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "output = '/content/dataset.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Step 2: Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(output)\n",
        "\n",
        "# Ensure the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Step 3: Create the model pipeline (features scaling + LinearRegression)\n",
        "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
        "\n",
        "# Step 4: Create a stream from the CSV file\n",
        "stream_data = stream.iter_csv(output, target=\"Stress\")\n",
        "\n",
        "# Step 5: Set up evaluation metrics (R², MSE, MAE using sklearn)\n",
        "metric_r2 = metrics.R2()\n",
        "\n",
        "# Initialize lists to track predictions and actual values for MSE and MAE\n",
        "predictions = []\n",
        "actual_values = []\n",
        "\n",
        "# Step 6: Train the model incrementally and evaluate performance\n",
        "for x, y in stream_data:\n",
        "    # Ensure that the data passed to `learn_one` is numeric\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    # Incremental learning\n",
        "    model.learn_one(x, y)\n",
        "\n",
        "    # Make predictions and store them for later evaluation\n",
        "    predicted_value = model.predict_one(x)\n",
        "    predictions.append(predicted_value)\n",
        "    actual_values.append(y)\n",
        "\n",
        "    # Update R² metric\n",
        "    metric_r2.update(y, predicted_value)\n",
        "\n",
        "# Step 7: Calculate and print the additional metrics: MSE and MAE\n",
        "mse_value = mean_squared_error(actual_values, predictions)\n",
        "mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "print(f\"Model R^2 score: {metric_r2.get()}\")\n",
        "print(f\"Model Mean Squared Error (MSE): {mse_value}\")\n",
        "print(f\"Model Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 8: Monitor the model over time (example for when new data arrives)\n",
        "def update_model_with_new_data(new_data):\n",
        "    \"\"\"This function can be called whenever new data arrives.\"\"\"\n",
        "    for x, y in new_data:\n",
        "        x = {k: float(v) for k, v in x.items()}\n",
        "        y = float(y)\n",
        "\n",
        "        # Incrementally update the model\n",
        "        model.learn_one(x, y)\n",
        "\n",
        "        # Make predictions and update metrics with new data\n",
        "        predicted_value = model.predict_one(x)\n",
        "        metric_r2.update(y, predicted_value)\n",
        "\n",
        "        # Update MSE and MAE\n",
        "        predictions.append(predicted_value)\n",
        "        actual_values.append(y)\n",
        "\n",
        "    # Calculate and print the new MSE and MAE after updating the model\n",
        "    mse_value = mean_squared_error(actual_values, predictions)\n",
        "    mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "    print(f\"Updated Model R^2 score: {metric_r2.get()}\")\n",
        "    print(f\"Updated Mean Squared Error (MSE): {mse_value}\")\n",
        "    print(f\"Updated Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 9: Save the model after training\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Step 10: Optionally, to load the model back later for further predictions\n",
        "# with open('model.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecQQIm1ZZ8ZE",
        "outputId": "02ad998b-4534-43a7-ecf8-8525ae07fadc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 7.07k/7.07k [00:00<00:00, 12.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model R^2 score: 0.3583192628676791\n",
            "Model Mean Squared Error (MSE): 107.37762142386455\n",
            "Model Mean Absolute Error (MAE): 5.174591285183506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-47-f8a219745774>:20: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df = df.apply(pd.to_numeric, errors='ignore')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gdown river pandas scikit-learn\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from river import linear_model\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Step 1: Download dataset from Google Drive (using file ID)\n",
        "url = 'https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "output = '/content/dataset.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Step 2: Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(output)\n",
        "\n",
        "# Ensure the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='ignore')\n",
        "\n",
        "# Step 3: Create the model pipeline (features scaling + LinearRegression)\n",
        "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
        "\n",
        "# Step 4: Create a stream from the CSV file\n",
        "stream_data = stream.iter_csv(output, target=\"Stress\")\n",
        "\n",
        "# Step 5: Set up evaluation metrics (R², MSE, MAE using sklearn)\n",
        "metric_r2 = metrics.R2()\n",
        "\n",
        "# Initialize lists to track predictions and actual values for MSE and MAE\n",
        "predictions = []\n",
        "actual_values = []\n",
        "\n",
        "# Step 6: Train the model incrementally and evaluate performance\n",
        "for x, y in stream_data:\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    # Incremental learning\n",
        "    model.learn_one(x, y)\n",
        "\n",
        "    # Make predictions and store them for later evaluation\n",
        "    predicted_value = model.predict_one(x)\n",
        "    predictions.append(predicted_value)\n",
        "    actual_values.append(y)\n",
        "\n",
        "    # Update R² metric\n",
        "    metric_r2.update(y, predicted_value)\n",
        "\n",
        "# Step 7: Calculate and print the additional metrics: MSE and MAE\n",
        "mse_value = mean_squared_error(actual_values, predictions)\n",
        "mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "print(f\"Model R^2 score: {metric_r2.get()}\")\n",
        "print(f\"Model Mean Squared Error (MSE): {mse_value}\")\n",
        "print(f\"Model Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 8: Monitor the model over time (example for when new data arrives)\n",
        "def update_model_with_new_data(new_data):\n",
        "    \"\"\"This function can be called whenever new data arrives.\"\"\"\n",
        "    for x, y in new_data:\n",
        "        x = {k: float(v) for k, v in x.items()}\n",
        "        y = float(y)\n",
        "\n",
        "        # Incrementally update the model\n",
        "        model.learn_one(x, y)\n",
        "\n",
        "        # Make predictions and update metrics with new data\n",
        "        predicted_value = model.predict_one(x)\n",
        "        metric_r2.update(y, predicted_value)\n",
        "\n",
        "        # Update MSE and MAE\n",
        "        predictions.append(predicted_value)\n",
        "        actual_values.append(y)\n",
        "\n",
        "    # Calculate and print the new MSE and MAE after updating the model\n",
        "    mse_value = mean_squared_error(actual_values, predictions)\n",
        "    mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "    print(f\"Updated Model R^2 score: {metric_r2.get()}\")\n",
        "    print(f\"Updated Mean Squared Error (MSE): {mse_value}\")\n",
        "    print(f\"Updated Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 9: Save the model after training\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Step 10: Optionally, to load the model back later for further predictions\n",
        "# with open('model.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pes115iXlO2C",
        "outputId": "af5df9f3-1521-477b-c5ab-44f37bd83570"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: river in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from river) (1.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 7.07k/7.07k [00:00<00:00, 13.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model R^2 score: 0.3583192628676791\n",
            "Model Mean Squared Error (MSE): 107.37762142386455\n",
            "Model Mean Absolute Error (MAE): 5.174591285183506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-49-05ca9841f9c7>:23: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
            "  df = df.apply(pd.to_numeric, errors='ignore')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from river import ensemble\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "import pandas as pd\n",
        "import gdown\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Download the dataset\n",
        "url = 'https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "output = '/content/dataset.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(output)\n",
        "\n",
        "# Ensure the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')  # 'coerce' converts invalid parsing to NaN\n",
        "df = df.fillna(0)  # Replace NaN values with 0 (or choose another strategy)\n",
        "\n",
        "# Create the model pipeline (features scaling + RandomForestRegressor)\n",
        "model = preprocessing.StandardScaler() | ensemble.RandomForestRegressor()\n",
        "\n",
        "# Create a stream from the CSV file\n",
        "stream_data = stream.iter_csv(output, target=\"Stress\")\n",
        "\n",
        "# Set up evaluation metrics (R², MSE, MAE)\n",
        "metric_r2 = metrics.R2()\n",
        "predictions = []\n",
        "actual_values = []\n",
        "\n",
        "# Train the model incrementally and evaluate performance\n",
        "for x, y in stream_data:\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    model.learn_one(x, y)  # Incremental learning\n",
        "    predicted_value = model.predict_one(x)\n",
        "    predictions.append(predicted_value)\n",
        "    actual_values.append(y)\n",
        "\n",
        "    # Update R² metric\n",
        "    metric_r2.update(y, predicted_value)\n",
        "\n",
        "# Calculate and print additional metrics: MSE and MAE\n",
        "mse_value = mean_squared_error(actual_values, predictions)\n",
        "mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "print(f\"Model R^2 score: {metric_r2.get()}\")\n",
        "print(f\"Model Mean Squared Error (MSE): {mse_value}\")\n",
        "print(f\"Model Mean Absolute Error (MAE): {mae_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "7DIAgxzCmLfC",
        "outputId": "bfa6b05f-a03f-42ce-8caf-cf330cf4a626"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 7.07k/7.07k [00:00<00:00, 13.7MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'river.ensemble' has no attribute 'RandomForestRegressor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-af478f3e0896>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Create the model pipeline (features scaling + RandomForestRegressor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Create a stream from the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'river.ensemble' has no attribute 'RandomForestRegressor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gdown river pandas scikit-learn\n",
        "\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from river import tree\n",
        "from river import preprocessing\n",
        "from river import metrics\n",
        "from river import stream\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Step 1: Download dataset from Google Drive (using file ID)\n",
        "url = 'https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev'\n",
        "output = '/content/dataset.csv'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Step 2: Load the CSV file into a Pandas DataFrame\n",
        "df = pd.read_csv(output)\n",
        "\n",
        "# Ensure the data types are correct (numeric columns should be floats)\n",
        "df = df.apply(pd.to_numeric, errors='coerce')  # 'coerce' converts invalid parsing to NaN\n",
        "df = df.fillna(0)  # Replace NaN values with 0 (or choose another strategy)\n",
        "\n",
        "# Step 3: Create the model pipeline (features scaling + HoeffdingTreeRegressor)\n",
        "model = preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor()\n",
        "\n",
        "# Step 4: Create a stream from the CSV file\n",
        "stream_data = stream.iter_csv(output, target=\"Stress\")\n",
        "\n",
        "# Step 5: Set up evaluation metrics (R², MSE, MAE)\n",
        "metric_r2 = metrics.R2()\n",
        "predictions = []\n",
        "actual_values = []\n",
        "\n",
        "# Step 6: Train the model incrementally and evaluate performance\n",
        "for x, y in stream_data:\n",
        "    x = {k: float(v) for k, v in x.items()}\n",
        "    y = float(y)\n",
        "\n",
        "    model.learn_one(x, y)  # Incremental learning\n",
        "    predicted_value = model.predict_one(x)\n",
        "    predictions.append(predicted_value)\n",
        "    actual_values.append(y)\n",
        "\n",
        "    # Update R² metric\n",
        "    metric_r2.update(y, predicted_value)\n",
        "\n",
        "# Step 7: Calculate and print additional metrics: MSE and MAE\n",
        "mse_value = mean_squared_error(actual_values, predictions)\n",
        "mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "print(f\"Model R^2 score: {metric_r2.get()}\")\n",
        "print(f\"Model Mean Squared Error (MSE): {mse_value}\")\n",
        "print(f\"Model Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 8: Monitor the model over time (example for when new data arrives)\n",
        "def update_model_with_new_data(new_data):\n",
        "    \"\"\"This function can be called whenever new data arrives.\"\"\"\n",
        "    for x, y in new_data:\n",
        "        x = {k: float(v) for k, v in x.items()}\n",
        "        y = float(y)\n",
        "\n",
        "        # Incrementally update the model\n",
        "        model.learn_one(x, y)\n",
        "\n",
        "        # Make predictions and update metrics with new data\n",
        "        predicted_value = model.predict_one(x)\n",
        "        metric_r2.update(y, predicted_value)\n",
        "\n",
        "        # Update MSE and MAE\n",
        "        predictions.append(predicted_value)\n",
        "        actual_values.append(y)\n",
        "\n",
        "    # Calculate and print the new MSE and MAE after updating the model\n",
        "    mse_value = mean_squared_error(actual_values, predictions)\n",
        "    mae_value = mean_absolute_error(actual_values, predictions)\n",
        "\n",
        "    print(f\"Updated Model R^2 score: {metric_r2.get()}\")\n",
        "    print(f\"Updated Mean Squared Error (MSE): {mse_value}\")\n",
        "    print(f\"Updated Mean Absolute Error (MAE): {mae_value}\")\n",
        "\n",
        "# Step 9: Save the model after training\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Step 10: Optionally, to load the model back later for further predictions\n",
        "# with open('model.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RgNNDdUmUPK",
        "outputId": "2876f98e-6a62-45fa-b492-11df0282ec15"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: river in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from river) (1.15.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WAO6YuxRRBi8A19GhBjSkb9CQJ-2aZev\n",
            "To: /content/dataset.csv\n",
            "100%|██████████| 7.07k/7.07k [00:00<00:00, 13.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model R^2 score: 0.7190628242189387\n",
            "Model Mean Squared Error (MSE): 47.011487113860326\n",
            "Model Mean Absolute Error (MAE): 3.292728110445836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}